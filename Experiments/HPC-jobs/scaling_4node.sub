#!/bin/bash
#BSUB -J scaling_4node
#BSUB -q hpcintro
#BSUB -n 96
#BSUB -R "span[ptile=24]"
#BSUB -R "rusage[mem=8GB]"
#BSUB -W 2:00
#BSUB -o logs/lsf/scaling_4node_%J.out
#BSUB -e logs/lsf/scaling_4node_%J.err

# =============================================================================
# Scaling Experiments: 4 nodes (96 cores)
# Binding: SPREAD (ppr:12:socket for even socket distribution)
# =============================================================================

module load mpi
cd $LS_SUBCWD

# Spread binding: 12 ranks per socket
MPIOPT="--map-by ppr:12:socket --bind-to core"

run_experiment() {
    local NP=$1
    local CONFIG=$2
    local EXTRA_ARGS="${@:3}"

    echo "=== Running: $CONFIG with $NP ranks ==="
    mpiexec $MPIOPT -n $NP uv run python run_solver.py \
        +experiment=$CONFIG \
        hydra/launcher=basic \
        n_ranks=$NP \
        $EXTRA_ARGS
}

# Strong Scaling: Jacobi & FMG (96 ranks)
run_experiment 96 scaling
run_experiment 96 fmg_scaling

# Weak Scaling: 64 ranks, N=1025
run_experiment 64 weak_scaling N=1025
run_experiment 64 fmg_scaling N=1025 experiment_name=fmg_weak_scaling

echo "Scaling 4-node completed"
