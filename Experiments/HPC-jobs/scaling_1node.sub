#!/bin/bash
#BSUB -J scaling_1node
#BSUB -q hpcintro
#BSUB -n 24
#BSUB -R "span[ptile=24]"
#BSUB -R "rusage[mem=8GB]"
#BSUB -W 4:00
#BSUB -o logs/lsf/scaling_1node_%J.out
#BSUB -e logs/lsf/scaling_1node_%J.err

# =============================================================================
# Scaling Experiments: 1 node (24 cores)
# Binding: SPREAD (ppr:12:socket for even socket distribution)
# Runs: Strong scaling (Jacobi + FMG), Weak scaling (Jacobi + FMG)
# =============================================================================

module load mpi
cd $LS_SUBCWD

# Spread binding: 12 ranks per socket
MPIOPT="--map-by ppr:12:socket --bind-to core"

run_experiment() {
    local NP=$1
    local CONFIG=$2
    local EXTRA_ARGS="${@:3}"

    echo "=== Running: $CONFIG with $NP ranks ==="
    mpiexec $MPIOPT -n $NP uv run python run_solver.py \
        +experiment=$CONFIG \
        hydra/launcher=basic \
        n_ranks=$NP \
        $EXTRA_ARGS
}

# Strong Scaling: Jacobi
for NP in 1 2 4 8 12 24; do
    run_experiment $NP scaling
done

# Strong Scaling: FMG
for NP in 1 2 4 8 12 24; do
    run_experiment $NP fmg_scaling
done

# Weak Scaling: Jacobi (1 rank, N=257)
run_experiment 1 weak_scaling N=257

# Weak Scaling: Jacobi (8 ranks, N=513)
run_experiment 8 weak_scaling N=513

# Weak Scaling: FMG (1 rank, N=257)
run_experiment 1 fmg_scaling N=257 experiment_name=fmg_weak_scaling

# Weak Scaling: FMG (8 ranks, N=513)
run_experiment 8 fmg_scaling N=513 experiment_name=fmg_weak_scaling

echo "Scaling 1-node completed"
