# ============================================================================
# LSF Experiment Configuration
# ============================================================================
# Format:
# GroupName:
#   lsf_options: [...]    # List of LSF bsub flags, can use placeholders {var}
#   command: str          # Full command template with placeholders for sweep/static args
#   static_args: { ... }  # Static arguments (always applied, can be used in command template)
#   sweep: { ... }        # Arguments to sweep over (cartesian product)
#   env_vars: { ... }     # Environment variables to set for the job (optional)

# Placeholders available for formatting:
#   - Any key from static_args or sweep
#   - job_name (generated uniquely per job)
#   - LSF_OUTPUT_DIR (absolute path to session-specific output directory for job logs)
#   - experiment_name (set to group name by default)

# ----------------------------------------------------------------------------
# Group 1: Single Node Strong Scaling
# ----------------------------------------------------------------------------
# Tests scaling on a single node (1-24 cores)
# Grid sizes chosen to give meaningful work per core
# ~5 min wall time with 100 iterations
SN_strong:
  lsf_options:
    - "-q hpc"
    - "-W 00:10"
    - "-n {ranks}"
    - "-J {job_name}"
    - "-o {LSF_OUTPUT_DIR}/{job_name}.out"
    - "-e {LSF_OUTPUT_DIR}/{job_name}.err"
    - "-R 'rusage[mem=4GB]'"
    - "-R 'span[hosts=1]'"

  command: >-
    cd $LSB_SUBCWD && mkdir -p {LSF_OUTPUT_DIR} &&
    module purge && module load mpi && uv sync &&
    mpiexec -n {ranks} uv run python Experiments/05-scaling/runner.py
    --N {N} --strategy {strategy} --communicator {communicator}
    --tol {tol} --max-iter {max_iter} --numba
    --job-name {job_name} --experiment-name Experiment-05-Scaling

  static_args:
    tol: 0.0
    max_iter: 100

  sweep:
    N: [128, 256, 512]
    ranks: [8, 16, 32, 64, 96]
    strategy: ["sliced", "cubic"]
    communicator: ["numpy", "custom"]

# ----------------------------------------------------------------------------
# Group 2: Multi Node Strong Scaling
# ----------------------------------------------------------------------------
# Tests scaling across multiple nodes (24+ cores)
# Larger grid to ensure enough work for many cores
# ~5-10 min wall time
MN_strong:
  lsf_options:
    - "-q hpc"
    - "-W 00:15"
    - "-n {ranks}"
    - "-J {job_name}"
    - "-o {LSF_OUTPUT_DIR}/{job_name}.out"
    - "-e {LSF_OUTPUT_DIR}/{job_name}.err"
    - "-R 'rusage[mem=4GB]'"
    - "-R 'span[ptile=24]'"

  command: >-
    cd $LSB_SUBCWD && mkdir -p {LSF_OUTPUT_DIR} &&
    module purge && module load mpi && uv sync &&
    mpiexec -n {ranks} uv run python Experiments/05-scaling/runner.py
    --N {N} --strategy {strategy} --communicator {communicator}
    --tol {tol} --max-iter {max_iter} --numba
    --job-name {job_name} --experiment-name Experiment-05-Scaling

  static_args:
    tol: 0.0
    max_iter: 100

  sweep:
    N: [128, 192]
    ranks: [24, 48, 96]
    strategy: ["sliced", "cubic"]
    communicator: ["numpy", "custom"]

# ----------------------------------------------------------------------------
# Group 3: Weak Scaling (Sliced)
# ----------------------------------------------------------------------------
# Problem size per processor stays constant
# For sliced (1D decomposition): N scales linearly with p
# p=1->N=32, p=2->N=64, p=4->N=128, p=8->N=256
SN_weak_sliced:
  lsf_options:
    - "-q hpc"
    - "-W 00:10"
    - "-n {ranks}"
    - "-J {job_name}"
    - "-o {LSF_OUTPUT_DIR}/{job_name}.out"
    - "-e {LSF_OUTPUT_DIR}/{job_name}.err"
    - "-R 'rusage[mem=4GB]'"
    - "-R 'span[hosts=1]'"

  command: >-
    cd $LSB_SUBCWD && mkdir -p {LSF_OUTPUT_DIR} &&
    module purge && module load mpi && uv sync &&
    mpiexec -n {ranks} uv run python Experiments/05-scaling/runner.py
    --N {N} --strategy sliced --communicator {communicator}
    --tol {tol} --max-iter {max_iter} --numba
    --job-name {job_name} --experiment-name Experiment-05-Scaling

  static_args:
    tol: 0.0
    max_iter: 50

  # Paired: N and ranks vary together (zipped, not cartesian)
  sweep_paired:
    N: [32, 64, 128, 256]
    ranks: [1, 2, 4, 8]

  # Regular sweep: cartesian product with communicator
  sweep:
    communicator: ["numpy", "custom"]

# ----------------------------------------------------------------------------
# Group 4: Weak Scaling (Cubic)
# ----------------------------------------------------------------------------
# For cubic (3D decomposition): N scales with p^(1/3)
# p=1->N=32, p=8->N=64, p=27->N=96
SN_weak_cubic:
  lsf_options:
    - "-q hpc"
    - "-W 00:10"
    - "-n {ranks}"
    - "-J {job_name}"
    - "-o {LSF_OUTPUT_DIR}/{job_name}.out"
    - "-e {LSF_OUTPUT_DIR}/{job_name}.err"
    - "-R 'rusage[mem=4GB]'"
    - "-R 'span[hosts=1]'"

  command: >-
    cd $LSB_SUBCWD && mkdir -p {LSF_OUTPUT_DIR} &&
    module purge && module load mpi && uv sync &&
    mpiexec -n {ranks} uv run python Experiments/05-scaling/runner.py
    --N {N} --strategy cubic --communicator {communicator}
    --tol {tol} --max-iter {max_iter} --numba
    --job-name {job_name} --experiment-name Experiment-05-Scaling

  static_args:
    tol: 0.0
    max_iter: 50

  # Paired: N and ranks vary together
  sweep_paired:
    N: [32, 64, 96]
    ranks: [1, 8, 27]

  sweep:
    communicator: ["numpy", "custom"]
