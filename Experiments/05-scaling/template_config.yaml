# ============================================================================
# LSF Scaling Experiment Configuration Template
# ============================================================================

type: strong # Options: strong, weak
script: Experiments/05-scaling/compute_scaling.py

# ============================================================================
# Global Parameters (Apply to all groups)
# ============================================================================
# Arguments passed to the python script
parameters:
  tol: 1e-6
  max-iter: 50000
  numba: true # Always use numba

# ============================================================================
# Global LSF Settings (Apply to all groups)
# ============================================================================
lsf:
  queue: hpc          # -q: Queue name
  walltime: "00:10"   # -W: Walltime (HH:MM)
  
  # Default resources
  resources:
    - "rusage[mem=4GB]"
  
  # Additional options
  exclusive: false    # -x: Exclusive node access

# ============================================================================
# Experiment Groups
# ============================================================================
groups:
  # --------------------------------------------------------------------------
  # Group 1: Single Node (Shared Memory)
  # --------------------------------------------------------------------------
  # Runs on a single host. Useful for small scale testing and debugging.
  - name: single_node
    lsf:
      resources:
        - "rusage[mem=4GB]"
        - "span[hosts=1]"  # FORCE execution on a single node
    
    # Sweep parameters generate one job per combination
    sweep:
      N: [64, 128]
      ranks: [1, 2, 4, 8, 16, 32]
      communicator: [numpy, custom]
      strategy: [sliced, cubic]

  # --------------------------------------------------------------------------
  # Group 2: Multi Node (Distributed Memory)
  # --------------------------------------------------------------------------
  # Runs across multiple nodes. 
  # 'ptile' defines how many MPI ranks to place per node.
  - name: multi_node
    lsf:
      walltime: "00:10"
      resources:
        - "rusage[mem=8GB]"
        - "span[ptile=24]" # Place 24 ranks per node (e.g. full node usage)
    
    # MPI options for process binding and mapping
    # Example: 2 processes per CPU socket (package)
    mpi_options: "--map-by ppr:2:package --bind-to core --report-bindings"
    
    sweep:
      N: [256]
      ranks: [48, 96] # Must be multiples of ptile (24) for efficient packing
      communicator: [custom, numpy]
      strategy: [cubic, sliced]
